[toc]

## 项目架构
go-zero, gRPC, k8s, redis, boltdb
TODO: 架构图

## 项目亮点分析
### 索引构建与多路召回
基于倒排索引及向量索引实现多路召回架构，通过引入**BM25相关性预计算**，显著降低搜索阶段的计算负载，提升整体召回效率；运用**Bitmap**优化文档集合的交并操作，并有效压缩倒排索引的存储空间(**平均压缩率50%**)。

#### 召回后的融合
1. 归一化
   * 倒排索引：Min-Max 归一化
   ```python
    bm25_scores = [doc.score for doc in bm25_docs]
    bm25_min, bm25_max = min(bm25_scores), max(bm25_scores)
    norm_bm25 = [(s - bm25_min) / (bm25_max - bm25_min + 1e-8) for s in bm25_scores]
    ```
   * 向量相似度同样归一化(返回的时候已经归一化了)
2. 加权分数合并(语义匹配的权重更高)
   * 倒排 * 0.6
   * 向量 * 0.4
3. 排序，返回 8 篇文档

向量索引

倒排索引返回 20-30 篇文档，向量索引返回 20-25 篇文档

#### TF-IDF
tf-idf(t,d)=tf(t,d)×idf(t)

* TF：一个词在当前文档中出现的频率。
* IDF：公式为 log(文档总数 / 包含该词的文档数 + 1)。衡量词条的普遍程度，在多少文档中出现，在越多文档中出现，IDF 值越低。
* TF-IDF 值高表示：该词在当前文档中频繁出现，但在整个文档集合中很少出现，因此具有较好的区分能力。

#### BM25
针对 TF-IDF 加入了长度归一化处理与词频非线性饱和处理


BM25 公式:

```shell
Score(D, Q) = Σ [ IDF(q_i) * ( TF(q_i, D) * (k1 + 1) ) / ( TF(q_i, D) + k1 * (1 - b + b * |D| / avgdl) ) ]
```
* IDF(q_i) - 逆文档频率项 IDF，衡量一个词的普遍重要性。一个词在所有文档中出现的越频繁，其 IDF 值越低。
  IDF(q_i) = log( 1 + (N - n(q_i) + 0.5) / (n(q_i) + 0.5) )
* N：文档集合中的总文档数。
* n(q_i)：包含词项 q_i 的文档数量。
* TF(q_i, D)：词项 q_i 在文档 D 中出现的次数。
* |D|：文档 D 的长度（通常用词数表示）。
* avgdl：整个文档集合中所有文档的平均长度。
* k1：一个可调参数，控制词频的饱和速率。 k1 越大，饱和越慢，词频的影响越大（通常取值在 1.2 到 2.0 之间）。 k1 = 0 意味着完全忽略词频。
* b：另一个可调参数，控制文档长度归一化的强度。
  b 在 0 到 1 之间。
  b = 1 表示 fully normalizing（完全进行长度归一化）。
  b = 0 表示完全不进行长度归一化。

#### pgSQL bitmap 存储倒排索引?
为什么采用 pgSQL?
* 好处：
  1. 对 bitmap 的存储支持的比较好
  2. 故障转移支持的比较好


压缩率 平均压缩率50%:
```shell
=== SUMMARY REPORT ===
| Case Name           | Set Size | List Size | Roaring Size | Bitset Size | Roaring vs List | Roaring vs Bitset |
|---------------------|----------|-----------|--------------|-------------|------------------|-------------------|
| Very Low Frequency  |       10 |        40 |         108 |    1250008 |         -170.0% |           100.0% |
| Low Frequency       |      100 |       400 |         816 |    1250008 |         -104.0% |            99.9% |
| Medium Frequency    |    10000 |     40000 |       21232 |    1250008 |           46.9% |            98.3% |
| High Frequency      |   100000 |    400000 |      201232 |    1250008 |           49.7% |            83.9% |
| Very High Frequency |  5000000 |  20000000 |     1254608 |    1250008 |           93.7% |            -0.4% |

```
#### 集合操作性能测试
| 测试项目-cpu核数            | 执行次数 | 每次操作耗时  | 每次操作分配的内存 | 每次操作的内存分配次数 |
| --------------------------- | -------- | ------------- | ------------------ | ---------------------- |
| BenchmarkIntersectList-8    | 1000     | 1125000 ns/op | 8192 B/op          | 1 allocs/op            |
| BenchmarkIntersectSet-8     | 3000     | 420000 ns/op  | 49152 B/op         | 1 allocs/op            |
| BenchmarkIntersectRoaring-8 | 10000    | 105000 ns/op  | 8192 B/op          | 1 allocs/op            |
| BenchmarkUnionList-8        | 1000     | 1250000 ns/op | 16384 B/op         | 1 allocs/op            |
| BenchmarkUnionSet-8         | 2000     | 680000 ns/op  | 98304 B/op         | 1 allocs/op            |
| BenchmarkUnionRoaring-8     | 10000    | 115000 ns/op  | 8192 B/op          | 1 allocs/op            |
|                             |          |               |                    |                        |

#### 倒排索引采用的是主从服务、读写分离
简单来说，处理主节点宕机的核心逻辑就是：

1. 发现它挂了（通过心跳）。 
2. 找个替身（通过选举，选数据最新的）。 
3. 通知大家新老板是谁（通过服务发现）。 
4. 让业务继续（新主节点开始服务）。
5. 宕机的旧主节点上线时，通过配置中心查询到主节点地址不是自己，变为从节点

### 分级搜索缓存
基于本地 **LFU** 及 **Redis** 构建搜索缓存。设计并实现了 **两级级缓存结构**，结合缓存智能过期、动态调整缓存时间管理热点query。

#### 两级缓存
* L1: 本地采用 LFU
  * hit_count > 3000: ttl = 3h
  * hit_count > 1000: ttl = 1h
  * 其它: ttl = 10min
* L2: 第二级采用 Redis 分布式缓存，通过 zset 管理分词及 hit_count 
  * TTL 设置为 7 天(缓存预热可以解决节假日缓存清空的问题)
  * 需要限制最大数量和内存，最大数量取总词语数量的 50%，内存为 2G

##### 搜索流程
阶段一：请求预处理与 Key 构造
* 对 query 做归一化处理(转小写、去空格等)
* 通过向量索引服务获取 top20 相似度的 query

阶段二：L1 本地缓存检查
* L1 检查：使用 score > 0.9 的 key 在 LFU 中搜索。
* L1 命中：直接返回 LLM 最终答案。流程结束。
* L1 未命中： 继续。

阶段三：L2 远程缓存检查
* 根据 score > 0.9 的 query 查找缓存
* 命中：写入 L1，返回答案
* 未命中：继续

阶段四：回源：双路召回与融合排序
* 倒排召回
* 向量召回
* 融合去重

阶段五：文档获取与 LLM 推理
* 根据融合去重后的 id 列表，获取对应的文档内容
* LLM 推理，返回答案

阶段六：缓存回写与同步
* 写入 L1 和 L2，先写 Redis，再写本地。优先保障共享缓存

### 消息队列解耦
用法: 收到索引构建请求后，将构建需求(doc_id)发送到 rabbitmq，倒排索引、向量索引及搜索服务同时消费

消息处理的幂等性: 索引服务(倒排、向量)分别通过 redis 缓存 message_id 实现消费的幂等性
message_id 如何设计? uuid + 时间戳

可靠性保证：rabbitmq 的消息确认机制，结合慢重试(通过延迟消息实现，初始重试时间为 1s，通过指数退避增加重试时间，最大重试次数为10次) + 死信队列保证任务构建的可靠性

延迟消息是通过延迟交换机绑定工作队列实现的，超过最大重试次数后放到死信队列

慢重试如何实现?

消息堆积：TODO

### 缓存预热
1. 捕获 Top Query： 编写一个简单的脚本，分析 上周五（或过去 7 天）的搜索日志，提取出 Top 10000 的高频搜索词。 
2. 定时重放： 在 周一早上 7:00 ~ 8:00（业务高峰前），通过脚本模拟请求，向搜索服务发起这几千次查询。 
3. 效果：
   * 这会强制触发后端计算，将结果加载到 L2 Redis 中。 
   * 甚至可以加载到应用节点的 L1 内存中（如果 L1 TTL 设置得当）。 
4. 进阶技巧： 预热时带上特殊的 Header（如 X-Warmup: true），不在日志中统计这些请求，以免污染 BI 数据。



#### 问题
* 为什么采用 Min-Max 归一化?
将不同索引的结果保持一个相对比例，公平对待不同索引的结果，且可以保留区分度。z-score 会压缩区分度，影响与向量分值的比较

* 为什么选择 BM25 进行相关性计算，而不选择其它关键词匹配算法?
  * BM25 对文档长度做了归一化处理，避免了长文档权重过高的问题。b 越大，文档长度影响越大。
  * 做了词频非线性饱和处理，词频越高，相关性增长越慢，最终趋于一个上限，防止相关性无限增加。k1 越大，词频对评分的影响越大

* 怎么解决文档 ID 稀疏性问题的?
  采用了 RoaringBitmap 进行 bitmap 的存储，其采用了分层分区策略

* 有没有做过集合操作的性能对比?
  * 操作速度是 List 的 10倍(且是已排序场景，不排序的复杂度是 O(N*N))
  * Set 的 2 倍

* 查询语句差一个字可能就会导致查询缓存失效，这种情况如何处理？
  根据语义相似性(向量相似度)处理，相似度 > 0.9 则认为相同

* 文档更新时如何让所有缓存(本地、redis)失效?
  消息队列通知

* 倒排索引中文档插入、更新、删除的流程
  * 插入: 对文档分词、预计算，设置分词的 bitmap，存储预计算信息和 bitmap
  * 删除: 对文档分词、更新预计算信息和分词的 bitmap
  * 更新: 先删除，再添加。如果要优化的化，可以考虑维护一个 delete_bitmap，用来标识删除的文档，查询时实际的 bitmap 与 delete_bitmap 进行集合差值计算操作

* 文档 id 的设计？
  * 分为 ExternalID 和 InternalID 
  * ExternalID 是由文档处理模块传递过来的，算法为 uuid + timestamp 生成字符串，再根据内部算法加密
  * InternalID 是倒排索引自建的整数 ID，通过一张表来维护
  * 建立 <ExternalID, InternalID> 的映射
  * 将 bitmap 的 id 分布稠密化

* 文档 id 列表数据量级有多少？
  * 10 万篇文档，去重后大概有 15 万个词
  * 词汇占内存大概 100000 * 50byte ≈ 5MB
  * 每个词大概出现在 150 篇文档中，倒排列表占内存 150000 * 150 * 8 ≈ 170MB
  * 用 bitmap 压缩后大概在 80MB
  * 加上预计算的数据(主要是TF(q_i, D)): 
    * 词项 q_i 在文档D中出现的次数 TF(q_i, D): 与倒排列表差不多，大概 160MB
  * 总计 240MB

* 倒排索引缓存
  * 将查询过的词都缓存起来

* 本地 LFU 和 Redis，限制数量、内存大小、过期时间？
  * 限制内存: 
    * 本地机器内存为 8G，留 512MB 作为缓存
    * Redis 机器内存为 8G，留 4G 作为缓存
  * 限制数量: 
    * 本地 10000 个词
    * Redis 30000-40000个
  * 过期时间: 
    * 本地高频 3h，中频 1h，低频 10min
    * Redis 设置 7 天

* 构建索引时，将所有缓存失效，高并发场景下，可能引发雪崩，这种情况怎么处理?
  * 一般来讲，索引的构建会选在业务低峰期，即使缓存失效，也不会导致服务压力过大。通过设置缓存版本号来失效
  * 如果需要考虑实时更新，可以:
    1. 后台启动线程，通过倒排索引反查哪些 query 会命中该文档，只失效这些 query;
    2. 在查询时，根据 query 的版本号来决定是否要采用缓存(先根据倒排索引确认是否失效,如果失效则回源，否则走缓存)
  * 限流兜底，30s 内最多 10000 请求

* 查询缓存时用向量索引服务再次查询显示的20条历史query，会不会影响缓存引入原本的意义呢？另外，如果原始 query 和历史 query 语义相似但答案不同，相似 query 的缓存结果可能导致错误响应，这个怎么解决?
  * 虽然引入了额外的向量查询开销，但是提升了缓存的命中率，且 query 的向量相似性计算并不会很大，只是牺牲了小部分性能
  * 针对错误响应，本身会针对向量相似性做过滤，必须要相关性 > 0.9 才做考虑
  * 可以考虑做日志埋点，以便监控误用率来优化(比如用户点击“答案不相关”的反馈是否集中在这些泛化缓存上)


* 缓存命中率
  * 本地 30%
  * Redis 70%

* qps
  * 高频 qps 在 3000
  * 平均在 1000

* 缓存预热的细节？从哪里读取日志？搜索词的有效性？预热会不胡影响业务性能？


* 如何避免缓存击穿、穿透、雪崩?
  TODO:
  缓存穿透:
  1. 对一些无用搜索进行排除: test, 111, 测试等
  2. 对未查询到的搜索缓存空值，且不进行热度评分
     缓存击穿: 热 query 的 score 比较高，不会突然过期
     缓存雪崩: 热 query 的 score 比较高，不会突然过期

* 缓存分布式锁的实现?
  TODO: 细节需要明确 redis setnx + expire + lua

* 缓存预热的搜索 query 是怎么收集的? 预热会不会对系统造成压力?
  * 搜索日志是应用层埋点来采集的，Top10,000 是根据过去 7 天的查询历史来统计的
  * 重放是进行 HTTP 请求调用实现的，本身会做速率限制，最大 100 QPS

* 预热的实际收益有没有对比过? 比如对比预热前后高峰时段的缓存命中率、P99 延迟？
  * 与空预热相比，P99 延迟下降 30%
  * 缓存命中率与平均表现差不多
  

* 如果未来业务增长 10 倍（QPS 从 1k → 10k，文档量从 10w → 100w），当前架构中最可能成为瓶颈的组件是什么？你会优先优化哪一部分？为什么？
  > 如果业务增长 10 倍，最可能的瓶颈是向量计算的吞吐能力。 
  > 
  > 当前架构中，每次新 query 都需调用外部向量服务生成 embedding，这在高 QPS 下会导致延迟飙升甚至超时。虽然我们有缓存，但冷 query 仍需实时计算。 
  > 
  > 我会优先优化这一路径：
  > 
  > 1. 对 query embedding 做本地缓存（Redis + 本地 LFU），减少重复计算； 
  > 2. 引入轻量级 embedding 模型部署在搜索服务本地（如 MiniLM），避免网络开销； 
  > 3. 对知识库场景做 query 模板泛化，预计算常见问法的向量，进一步降低在线计算量。
  > 
  >至于缓存容量问题，由于业务 query 高度集中，Top 10k 覆盖率高，可通过动态调整缓存规模和 TTL 策略应对，属于可扩展性问题，而非性能硬瓶颈。


## 排障经验
TODO

